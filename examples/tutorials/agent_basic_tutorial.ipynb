{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWgEkOAO9OVz"
      },
      "source": [
        "# Basic Agent Tutorial in Concordia\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCrPnXpVpaoy"
      },
      "source": [
        "This tutorial walks you through how to create your very first agent with Concordia.\n",
        "\n",
        "In a full Concordia experiment you'd have a _Game Master_ directing the narrative and deciding what happens in the environment, and some _agents_ interacting with the environment and each other. For this tutorial, we will focus only on the agents, as **you**, the user, will act like the Game Master.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/google-deepmind/concordia/blob/main/examples/tutorials/agent_basic_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NXAjT5XO2IQc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/yannikkellerde/miniconda3/envs/concordia/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "# @title Imports and initialization\n",
        "\n",
        "import sentence_transformers\n",
        "\n",
        "from concordia import typing\n",
        "from concordia.typing import entity\n",
        "\n",
        "from concordia.associative_memory import associative_memory\n",
        "from concordia.language_model import gpt_model\n",
        "from concordia.language_model import language_model\n",
        "from concordia.utils.create_azure_model import load_azure_model\n",
        "from concordia.utils.request_log_handler import load_request_log\n",
        "\n",
        "# The memory will use a sentence embedder for retrievel, so we download one from\n",
        "# Hugging Face.\n",
        "_embedder_model = sentence_transformers.SentenceTransformer(\n",
        "    'sentence-transformers/all-mpnet-base-v2'\n",
        ")\n",
        "embedder = lambda x: _embedder_model.encode(x, show_progress_bar=False)\n",
        "\n",
        "model = load_azure_model(env_file='~/.env')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6WFuGfinCXQ"
      },
      "source": [
        "# Building a dummy agent\n",
        "\n",
        "We will start by creating a dummy agent that just always tries to grab an apple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M8GKlpnm00No"
      },
      "outputs": [],
      "source": [
        "class DummyAgent(entity.Entity):\n",
        "\n",
        "  @property\n",
        "  def name(self) -> str:\n",
        "    return \"Dummy\"\n",
        "\n",
        "  def act(self, action_spec=entity.DEFAULT_ACTION_SPEC) -> str:\n",
        "    return \"Dummy attempts to grab an apple.\"\n",
        "\n",
        "  def observe(\n",
        "      self,\n",
        "      observation: str,\n",
        "  ) -> None:\n",
        "    pass\n",
        "\n",
        "\n",
        "agent = DummyAgent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XTAZR5MK7xzT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Dummy attempts to grab an apple.'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.act()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t40TS7W48iqn"
      },
      "source": [
        "Alright! We have our first agent... although not a terribly exciting one.\n",
        "\n",
        "Next let's create a very simple agent backed by the LLM.\n",
        "\n",
        "# Simple LLM Agent\n",
        "\n",
        "This agent remembers the last 5 observations, and acts by asking itself `\"What should you do next?\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Csdb6hfp803o"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "\n",
        "def make_prompt(deque: collections.deque[str]) -> str:\n",
        "  \"\"\"Makes a string prompt by joining all observations, one per line.\"\"\"\n",
        "  return \"\\n\".join(deque)\n",
        "\n",
        "\n",
        "class SimpleLLMAgent(entity.Entity):\n",
        "\n",
        "  def __init__(self, model: language_model.LanguageModel):\n",
        "    self._model = model\n",
        "    # Container (circular queue) for observations.\n",
        "    self._memory = collections.deque(maxlen=5)\n",
        "\n",
        "  @property\n",
        "  def name(self) -> str:\n",
        "    return \"Alice\"\n",
        "\n",
        "  def act(self, action_spec=entity.DEFAULT_ACTION_SPEC) -> str:\n",
        "    prompt = make_prompt(self._memory)\n",
        "    print(f\"*****\\nDEBUG: {prompt}\\n*****\")\n",
        "    return self._model.sample_text(\n",
        "        \"You are a person.\\n\"\n",
        "        f\"Your name is {self.name} and your recent observations are:\\n\"\n",
        "        f\"{prompt}\\nWhat should you do next?\"\n",
        "    )\n",
        "\n",
        "  def observe(\n",
        "      self,\n",
        "      observation: str,\n",
        "  ) -> None:\n",
        "    # Push a new observation into the memory, if there are too many, the oldest\n",
        "    # one will be automatically dropped.\n",
        "    self._memory.append(observation)\n",
        "\n",
        "\n",
        "agent = SimpleLLMAgent(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WkjYDbGT-1nf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*****\n",
            "DEBUG: You are in a room.\n",
            "The room has only a table in it.\n",
            "On the table there is a single apple.\n",
            "The apple is shinny red and looks absolutely irresistible!\n",
            "*****\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'I should approach the table, pick up the shiny red apple, and examine it closely. After that, I might consider taking a bite to see how it tastes.'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.observe(\"You are in a room.\")\n",
        "agent.observe(\"The room has only a table in it.\")\n",
        "agent.observe(\"On the table there is a single apple.\")\n",
        "agent.observe(\"The apple is shinny red and looks absolutely irresistible!\")\n",
        "agent.act()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What were the actual requests sent to the LLM?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[{'role': 'system', 'content': 'You always continue sentences provided by the user and you never repeat what the user already said.'}, {'role': 'user', 'content': 'Question: Is Jake a turtle?\\nAnswer: Jake is '}, {'role': 'assistant', 'content': 'not a turtle.'}, {'role': 'user', 'content': 'Question: What is Priya doing right now?\\nAnswer: Priya is currently '}, {'role': 'assistant', 'content': 'sleeping.'}, {'role': 'user', 'content': 'You are a person.\\nYour name is Alice and your recent observations are:\\nYou are in a room.\\nThe room has only a table in it.\\nOn the table there is a single apple.\\nThe apple is shinny red and looks absolutely irresistible!\\nWhat should you do next?'}, {'role': 'assistant (response)', 'content': 'I should approach the table, pick up the shiny red apple, and examine it closely. After that, I might consider taking a bite to see how it tastes.'}]]\n"
          ]
        }
      ],
      "source": [
        "request_log = load_request_log()\n",
        "print(request_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The requests include a system message and example prompts and answers. The prompt that is actually asked for can be found the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are a person.\n",
            "Your name is Alice and your recent observations are:\n",
            "You are in a room.\n",
            "The room has only a table in it.\n",
            "On the table there is a single apple.\n",
            "The apple is shinny red and looks absolutely irresistible!\n",
            "What should you do next?\n"
          ]
        }
      ],
      "source": [
        "print(request_log[-1][-2][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uBbVbd5K3OZ"
      },
      "source": [
        "Alright! We have an agent that takes observations and attempts actions.\n",
        "\n",
        "## Limitations of the `SimpleLLMAgent`\n",
        "\n",
        "While useful, the `SimpleLLMAgent` has some severe limitations. An obvious one is that if we push too many observations, we will lose them from context. We can increase the memory, but we are limited to the size of the LLM's context window, which, despite current models increasing this significantly from the times of 8-10k tokens, you can imagine a long running agent to run into this limit.\n",
        "\n",
        "Here's a toy example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UlnNWdDZ1sl_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*****\n",
            "DEBUG: You are in a room.\n",
            "The room has only a table in it.\n",
            "On the table there are two fruits: an apple and a banana.\n",
            "The apple is shinny red and looks absolutely irresistible!\n",
            "The banana is slightly past its prime.\n",
            "*****\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'I should reach for the shiny red apple, as it looks more appealing and fresh compared to the banana. Enjoying a crisp, delicious apple would be a great choice!'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent = SimpleLLMAgent(model)\n",
        "\n",
        "agent.observe(\"You absolutely hate apples and would never willingly eat them.\")\n",
        "agent.observe(\"You don't particularly like bananas.\")\n",
        "# Only the next 5 observations will be kept, pushing out critical information!\n",
        "agent.observe(\"You are in a room.\")\n",
        "agent.observe(\"The room has only a table in it.\")\n",
        "agent.observe(\"On the table there are two fruits: an apple and a banana.\")\n",
        "agent.observe(\"The apple is shinny red and looks absolutely irresistible!\")\n",
        "agent.observe(\"The banana is slightly past its prime.\")\n",
        "agent.act()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J_b6Iq2AqDI"
      },
      "source": [
        "We can fix this by adding a better memory to the agent. The `AssociativeMemory` saves _all_ observations, and does retrieval of semantically relevant memories on request. So, our agent becomes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "C8WHHbinBKOr"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "\n",
        "def make_prompt_associative_memory(\n",
        "    memory: associative_memory.AssociativeMemory,\n",
        ") -> str:\n",
        "  \"\"\"Makes a string prompt by joining all observations, one per line.\"\"\"\n",
        "  recent_memories_list = memory.retrieve_recent(5)\n",
        "  recent_memories_set = set(recent_memories_list)\n",
        "  recent_memories = \"\\n\".join(recent_memories_set)\n",
        "\n",
        "  relevant_memories_list = []\n",
        "  for recent_memory in recent_memories_list:\n",
        "    # Retrieve 3 memories that are relevant to the recent memory.\n",
        "    relevant = memory.retrieve_associative(recent_memory, 3, add_time=False)\n",
        "    for mem in relevant:\n",
        "      # Make sure that we only add memories that are _not_ already in the recent\n",
        "      # ones.\n",
        "      if mem not in recent_memories_set:\n",
        "        relevant_memories_list.append(mem)\n",
        "\n",
        "  relevant_memories = \"\\n\".join(relevant_memories_list)\n",
        "  return (\n",
        "      f\"Your recent memories are:\\n{recent_memories}\\n\"\n",
        "      f\"Relevant memories from your past:\\n{relevant_memories}\\n\"\n",
        "  )\n",
        "\n",
        "\n",
        "class SimpleLLMAgentWithAssociativeMemory(entity.Entity):\n",
        "\n",
        "  def __init__(self, model: language_model.LanguageModel, embedder):\n",
        "    self._model = model\n",
        "    # The associative memory of the agent. It uses a sentence embedder to\n",
        "    # retrieve on semantically relevant memories.\n",
        "    self._memory = associative_memory.AssociativeMemory(embedder)\n",
        "\n",
        "  @property\n",
        "  def name(self) -> str:\n",
        "    return \"Alice\"\n",
        "\n",
        "  def act(self, action_spec=entity.DEFAULT_ACTION_SPEC) -> str:\n",
        "    prompt = make_prompt_associative_memory(self._memory)\n",
        "    print(f\"*****\\nDEBUG: {prompt}\\n*****\")\n",
        "    return self._model.sample_text(\n",
        "        \"You are a person.\\n\"\n",
        "        f\"Your name is {self.name}.\\n\"\n",
        "        f\"{prompt}\\n\"\n",
        "        \"What should you do next?\"\n",
        "    )\n",
        "\n",
        "  def observe(\n",
        "      self,\n",
        "      observation: str,\n",
        "  ) -> None:\n",
        "    # Push a new observation into the memory, if there are too many, the oldest\n",
        "    # one will be automatically dropped.\n",
        "    self._memory.add(observation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tZ7m58eTD4k8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*****\n",
            "DEBUG: Your recent memories are:\n",
            "The apple is shinny red and looks absolutely irresistible!\n",
            "You are in a room.\n",
            "On the table there are two fruits: an apple and a banana.\n",
            "The banana is slightly past its prime.\n",
            "The room has only a table in it.\n",
            "Relevant memories from your past:\n",
            "You absolutely hate apples and would never willingly eat them.\n",
            "You don't particularly like bananas.\n",
            "\n",
            "*****\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"You should take a closer look at the banana to see if it's still edible, and if so, consider eating it instead of the apple.\""
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent = SimpleLLMAgentWithAssociativeMemory(model, embedder)\n",
        "\n",
        "agent.observe(\"You absolutely hate apples and would never willingly eat them.\")\n",
        "agent.observe(\"You don't particularly like bananas.\")\n",
        "# Only the next 5 observations will be retrieved as \"recent memories\"\n",
        "agent.observe(\"You are in a room.\")\n",
        "agent.observe(\"The room has only a table in it.\")\n",
        "agent.observe(\"On the table there are two fruits: an apple and a banana.\")\n",
        "agent.observe(\"The apple is shinny red and looks absolutely irresistible!\")\n",
        "agent.observe(\"The banana is slightly past its prime.\")\n",
        "agent.act()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlulNd5qGBeP"
      },
      "source": [
        "With a better memory, Alice should not eat the apple. She'll be able to remember she hates apples, and isn't super keen on bananas either. So she might choose to eat the banana, or just leave the room, or whatever else.\n",
        "\n",
        "# The Entity-Component system\n",
        "\n",
        "In the example above we are using an `AssociativeMemory` that we didn't have to implement, that's good. But now imagine we want to add some functionality for our agent to reason about how it is feeling at the moment. Maybe they are hungy because it hasn't eaten in a while, so they would eat the banana. We can easily do that by extending the class above, but it gets cumbersome and leads to a lot of forking code!\n",
        "\n",
        "Instead of forking, we will be building agents using components. The idea is that an `Entity` is something that exist (explicitly, we'll talk about that later on) in the environment, but its functionality is controlled by adding components to it. This is a pattern used in many game engines called an [Entity-Component-System](https://en.wikipedia.org/wiki/Entity_component_system).\n",
        "\n",
        "You can think of components as a piece of the thought process of the agent. All components, together, provide the full information that is used for the agent to act in a situation.\n",
        "\n",
        "In this way, any modular piece of functionality in the entity can be easily reused in other agents without having to fork. So, for example, a component that retrieves relevant memories given recent observations should be useful in our example above, and in many other agents. So we create a component to handle this.\n",
        "\n",
        "Now you are ready for the next tutorial: [Agent tutorial](https://colab.research.google.com/github/google-deepmind/concordia/blob/main/examples/tutorials/agent_components_tutorial.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeEx-oLp_U0K"
      },
      "source": [
        "```\n",
        "Copyright 2024 DeepMind Technologies Limited.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
