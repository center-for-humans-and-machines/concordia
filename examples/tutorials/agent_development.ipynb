{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKg27EKlM98K"
      },
      "source": [
        "# Agent development colab\n",
        "\n",
        "This is a notebook for agent development. It configures the agent using components and runs it on a selected environment. The colab presents a prompt engineering view of the agent and makes it easy to configure components with minimal coding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esFO3miE3s41"
      },
      "source": [
        "\u003ca href=\"https://colab.research.google.com/github/google-deepmind/concordia/blob/main/examples/tutorials/agent_development.ipynb\" target=\"_parent\"\u003e\u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/\u003e\u003c/a\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2TwJrZ08wXz"
      },
      "source": [
        "## Setup and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_JQ_dJDpBVC"
      },
      "outputs": [],
      "source": [
        "!pip install -r https://raw.githubusercontent.com/google-deepmind/concordia/main/examples/requirements.txt\n",
        "!pip install --ignore-requires-python git+https://github.com/google-deepmind/concordia.git\n",
        "\n",
        "#@markdown Here we check if this is being ran in colab or codespaces. If running a colab, we clone the repository to get the example environments\n",
        "import os\n",
        "import sys\n",
        "import pathlib\n",
        "\n",
        "if os.environ.get('CODESPACES') == 'true':\n",
        "  print(\"Running in Codespaces\")\n",
        "  concordia_root_dir = pathlib.Path.cwd().parent.parent.resolve()\n",
        "  sys.path.append(f'{concordia_root_dir}')\n",
        "elif 'google.colab' in sys.modules:\n",
        "  print(\"Running in Colab\")\n",
        "  !git clone https://github.com/google-deepmind/concordia.git\n",
        "  sys.path.append('/content/concordia/')\n",
        "else:\n",
        "  print(\"Running in a different environment, please make sure to add root directory of examples to with sys.path.append\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qLG5ExLqpWa"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import importlib\n",
        "import os\n",
        "import pathlib\n",
        "import sys\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "from concordia.language_model import amazon_bedrock_model\n",
        "from concordia.language_model import google_aistudio_model\n",
        "from concordia.language_model import gpt_model\n",
        "from concordia.language_model import langchain_ollama_model\n",
        "from concordia.language_model import mistral_model\n",
        "from concordia.language_model import no_language_model\n",
        "from concordia.language_model import ollama_model\n",
        "from concordia.language_model import pytorch_gemma_model\n",
        "from concordia.utils import measurements as measurements_lib\n",
        "import sentence_transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6YO41FyuwOO"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4peKcaQuwOP"
      },
      "outputs": [],
      "source": [
        "# @title Parameters (edit this cell)\n",
        "\n",
        "# Pick ENVIRONMENT_NAME from the factories in concordia/factory/environment.\n",
        "ENVIRONMENT_NAME = 'pub_coordination'\n",
        "# Pick from {openai, mistral}. Feel free to implement more!\n",
        "API_TYPE = 'mistral'\n",
        "# Pick  a specific model e.g. gpt-4o if openai or codestral-latest if mistral.\n",
        "# You can select any model listed at https://platform.openai.com/docs/models\n",
        "# if API_TYPE is openai or https://docs.mistral.ai/getting-started/models/ when\n",
        "# API_TYPE is mistral. Feel free to implement more!\n",
        "MODEL_NAME = 'codestral-latest'\n",
        "# Select an embedder by specifying one of the sentence transformer embedding\n",
        "# models listed at https://huggingface.co/sentence-transformers.\n",
        "EMBEDDER_NAME = 'all-mpnet-base-v2'\n",
        "# To debug without spending money on API calls, set DISABLE_LANGUAGE_MODEL=True.\n",
        "DISABLE_LANGUAGE_MODEL = False  # @param {type: 'boolean'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8G9o40NuwOP"
      },
      "source": [
        "## Load the environment config with importlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvZYDlOmuwOP"
      },
      "outputs": [],
      "source": [
        "# @title Load the agent config with importlib\n",
        "\n",
        "# Load the environment config with importlib\n",
        "IMPORT_ENV_BASE_DIR = 'examples.modular.environment'\n",
        "simulation = importlib.import_module(\n",
        "    f'{IMPORT_ENV_BASE_DIR}.{ENVIRONMENT_NAME}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brdgSD2NuwOQ"
      },
      "source": [
        "## Language Model setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ez6153pSuwOQ"
      },
      "outputs": [],
      "source": [
        "# @title Language Model setup\n",
        "\n",
        "if DISABLE_LANGUAGE_MODEL:\n",
        "  model = no_language_model.NoLanguageModel()\n",
        "elif API_TYPE == 'amazon_bedrock':\n",
        "  model = amazon_bedrock_model.AmazonBedrockLanguageModel(MODEL_NAME)\n",
        "elif API_TYPE == 'google_aistudio_model':\n",
        "  model = google_aistudio_model.GoogleAIStudioLanguageModel(MODEL_NAME)\n",
        "elif API_TYPE == 'langchain_ollama':\n",
        "  model = langchain_ollama_model.LangchainOllamaLanguageModel(MODEL_NAME)\n",
        "elif API_TYPE == 'mistral':\n",
        "  model = mistral_model.MistralLanguageModel(MODEL_NAME)\n",
        "elif API_TYPE == 'ollama':\n",
        "  model = ollama_model.OllamaLanguageModel(MODEL_NAME)\n",
        "elif API_TYPE == 'openai':\n",
        "  model = gpt_model.GptLanguageModel(MODEL_NAME)\n",
        "elif API_TYPE == 'pytorch_gemma':\n",
        "  model = pytorch_gemma_model.PyTorchGemmaLanguageModel(MODEL_NAME)\n",
        "else:\n",
        "  raise ValueError(f'Unrecognized api type: {API_TYPE}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb_VFDtvuwOQ"
      },
      "source": [
        "## Setup sentence encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UE-enMPMuwOQ"
      },
      "outputs": [],
      "source": [
        "# @title Setup sentence encoder\n",
        "_embedder_model = sentence_transformers.SentenceTransformer(\n",
        "    f'sentence-transformers/{EMBEDDER_NAME}')\n",
        "embedder = lambda x: _embedder_model.encode(x, show_progress_bar=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1CQOKB0M98K"
      },
      "source": [
        "# Building an agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uUVB1z7M98K"
      },
      "outputs": [],
      "source": [
        "#@title Imports for agent building\n",
        "\n",
        "from concordia.agents import entity_agent_with_logging\n",
        "from concordia.associative_memory import associative_memory\n",
        "from concordia.associative_memory import formative_memories\n",
        "from concordia.clocks import game_clock\n",
        "from concordia.components import agent as agent_components\n",
        "from concordia.language_model import language_model\n",
        "from concordia.memory_bank import legacy_associative_memory\n",
        "from concordia.utils import measurements as measurements_lib\n",
        "from concordia.components.agent import question_of_recent_memories\n",
        "from typing import Sequence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxdnIvDLM98K"
      },
      "source": [
        "We are going to show how we to different kinds of agents by using a QuestionOfRecentMemories components. This type of component proceeds by first retrieving recent memories and then asking a question. For example, the question could be \"What kind of person is {agent_name}?\" or \"What is the rational thing to do next?\" and so on. The answers to these questions will condition the agents action, thereby defining its behavior.\n",
        "\n",
        "Important notes:\n",
        "- All text should refer to the agent in third person, without using \"I\", \"me\", \"mine\" and so on.\n",
        "- A special string {agent_name} will be automatically replaced with the agent's actual name during simulation (e.g. Alice).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1FQOUHdM98K"
      },
      "outputs": [],
      "source": [
        "#@markdown Each question is a class that inherits from QuestionOfRecentMemories\n",
        "class Question1(question_of_recent_memories.QuestionOfRecentMemories):\n",
        "  \"\"\"This component answers the question 'what kind of person is the agent?'.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      agent_name:str,\n",
        "      **kwargs,\n",
        "  ):\n",
        "    #@markdown {agent_name} will be automatically replaced with the name of the specific agent\n",
        "    question = 'Given the above, what kind of person is {agent_name}?' #@param {\"type\":\"string\"}\n",
        "    #@markdown The answer will have to start with this prefix\n",
        "    answer_prefix = '{agent_name} is ' #@param {\"type\":\"string\"}\n",
        "    #@markdown Flag that defines whether the answer will be added to memory\n",
        "    add_to_memory = True # @param {\"type\":\"boolean\"}\n",
        "    #@markdown If yes, the memory will start with this tag\n",
        "    memory_tag = '[self reflection]' # @param {\"type\":\"string\"}\n",
        "    question_with_name = question.format(agent_name=agent_name)\n",
        "    super().__init__(\n",
        "        pre_act_key=f'\\nQuestion: {question_with_name}\\nAnswer',\n",
        "        question=question,\n",
        "        answer_prefix=answer_prefix,\n",
        "        add_to_memory=add_to_memory,\n",
        "        memory_tag=memory_tag,\n",
        "        components={},\n",
        "        **kwargs,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKbalFVbM98K"
      },
      "outputs": [],
      "source": [
        "#@markdown We can add the value of other components to the context of the question. Notice, how Question2 depends on Observation and ObservationSummary. The names of the classes of the contextualising components have to be passed as \"components\" argument.\n",
        "class Question2(question_of_recent_memories.QuestionOfRecentMemories):\n",
        "  \"\"\"This component answers 'which action is best for achieving my goal?'.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      agent_name:str,\n",
        "      **kwargs,\n",
        "  ):\n",
        "    question = 'Given the statements above, what kind of situation is {agent_name} in right now?' #@param {\"type\":\"string\"}\n",
        "    answer_prefix = '{agent_name} is currently ' #@param {\"type\":\"string\"}\n",
        "    add_to_memory = False # @param {\"type\":\"boolean\"}\n",
        "    memory_tag = '[situation reflection]' # @param {\"type\":\"string\"}\n",
        "    question_with_name = question.format(agent_name=agent_name)\n",
        "\n",
        "    super().__init__(\n",
        "        pre_act_key=f'\\nQuestion: {question_with_name}\\nAnswer',\n",
        "        question=question,\n",
        "        answer_prefix=answer_prefix,\n",
        "        add_to_memory=add_to_memory,\n",
        "        memory_tag=memory_tag,\n",
        "        #@markdown The key is the name of the component class and the key is the prefix with which it will appear in the context of this component. Be careful if you are going to edit this field, it should be a valid dictionary.\n",
        "        components={'Observation': '\\nObservation', 'ObservationSummary': '\\nSummary of recent observations',}, #@param\n",
        "\n",
        "        **kwargs,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEH13Zw3M98K"
      },
      "outputs": [],
      "source": [
        "#@markdown We can also have the questions depend on each other. Here, the answer to Question3 is contextualised by answers to Question1 and Question2\n",
        "class Question3(question_of_recent_memories.QuestionOfRecentMemories):\n",
        "  \"\"\"What would a person like the agent do in a situation like this?\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      agent_name:str,\n",
        "      **kwargs):\n",
        "    question = 'What would a person like {agent_name} do in a situation like this?' #@param {\"type\":\"string\"}\n",
        "    answer_prefix = '{agent_name} would ' #@param {\"type\":\"string\"}\n",
        "    add_to_memory = True # @param {\"type\":\"boolean\"}\n",
        "    memory_tag = '[intent reflection]' # @param {\"type\":\"string\"}\n",
        "\n",
        "    question_with_name = question.format(agent_name=agent_name)\n",
        "\n",
        "    super().__init__(\n",
        "        pre_act_key=f'\\nQuestion: {question_with_name}\\nAnswer',\n",
        "        question=question,\n",
        "        answer_prefix=answer_prefix,\n",
        "        add_to_memory=add_to_memory,\n",
        "        memory_tag=memory_tag,\n",
        "        components={'Question1': '\\nQuestion: What kind of person is {agent_name}?\\nAnswer', 'Question2': '\\nQuestion: What kind of situation is {agent_name} in right now?\\nAnswer',}, #@param\n",
        "        **kwargs,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8eNNO1rM98K"
      },
      "outputs": [],
      "source": [
        "#@markdown This function creates the components\n",
        "\n",
        "def _make_question_components(\n",
        "    agent_name:str,\n",
        "    measurements: measurements_lib.Measurements,\n",
        "    model: language_model.LanguageModel,\n",
        "    clock: game_clock.MultiIntervalClock,\n",
        ") -\u003e Sequence[question_of_recent_memories.QuestionOfRecentMemories]:\n",
        "\n",
        "  question_1 = Question1(\n",
        "      agent_name=agent_name,\n",
        "      model=model,\n",
        "      logging_channel=measurements.get_channel('Question_1').on_next,\n",
        "  )\n",
        "  question_2 = Question2(\n",
        "      agent_name=agent_name,\n",
        "      model=model,\n",
        "      clock_now=clock.now,\n",
        "      logging_channel=measurements.get_channel('Question_2').on_next,\n",
        "  )\n",
        "  question_3 = Question3(\n",
        "      agent_name=agent_name,\n",
        "      model=model,\n",
        "      clock_now=clock.now,\n",
        "      logging_channel=measurements.get_channel('Question_3').on_next,\n",
        "  )\n",
        "\n",
        "  return (question_1, question_2, question_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2W9WkK1TM98K"
      },
      "outputs": [],
      "source": [
        "def _get_class_name(object_: object) -\u003e str:\n",
        "  return object_.__class__.__name__\n",
        "\n",
        "#@markdown This function builds the agent using the components defined above. It also adds core components that are useful for every agent, like observations, time display, recenet memories.\n",
        "\n",
        "def build_agent(\n",
        "    config: formative_memories.AgentConfig,\n",
        "    model: language_model.LanguageModel,\n",
        "    memory: associative_memory.AssociativeMemory,\n",
        "    clock: game_clock.MultiIntervalClock,\n",
        "    update_time_interval: datetime.timedelta,\n",
        ") -\u003e entity_agent_with_logging.EntityAgentWithLogging:\n",
        "  \"\"\"Build an agent.\n",
        "\n",
        "  Args:\n",
        "    config: The agent config to use.\n",
        "    model: The language model to use.\n",
        "    memory: The agent's memory object.\n",
        "    clock: The clock to use.\n",
        "    update_time_interval: Agent calls update every time this interval passes.\n",
        "\n",
        "  Returns:\n",
        "    An agent.\n",
        "  \"\"\"\n",
        "  del update_time_interval\n",
        "  if not config.extras.get('main_character', False):\n",
        "    raise ValueError(\n",
        "        'This function is meant for a main character '\n",
        "        'but it was called on a supporting character.'\n",
        "    )\n",
        "\n",
        "  agent_name = config.name\n",
        "\n",
        "  raw_memory = legacy_associative_memory.AssociativeMemoryBank(memory)\n",
        "\n",
        "  measurements = measurements_lib.Measurements()\n",
        "  instructions = agent_components.instructions.Instructions(\n",
        "      agent_name=agent_name,\n",
        "      logging_channel=measurements.get_channel('Instructions').on_next,\n",
        "  )\n",
        "\n",
        "  time_display = agent_components.report_function.ReportFunction(\n",
        "      function=clock.current_time_interval_str,\n",
        "      pre_act_key='\\nCurrent time',\n",
        "      logging_channel=measurements.get_channel('TimeDisplay').on_next,\n",
        "  )\n",
        "\n",
        "  observation_label = '\\nObservation'\n",
        "  observation = agent_components.observation.Observation(\n",
        "      clock_now=clock.now,\n",
        "      timeframe=clock.get_step_size(),\n",
        "      pre_act_key=observation_label,\n",
        "      logging_channel=measurements.get_channel('Observation').on_next,\n",
        "  )\n",
        "  observation_summary_label = 'Summary of recent observations'\n",
        "  observation_summary = agent_components.observation.ObservationSummary(\n",
        "      model=model,\n",
        "      clock_now=clock.now,\n",
        "      timeframe_delta_from=datetime.timedelta(hours=4),\n",
        "      timeframe_delta_until=datetime.timedelta(hours=0),\n",
        "      pre_act_key=observation_summary_label,\n",
        "      logging_channel=measurements.get_channel('ObservationSummary').on_next,\n",
        "  )\n",
        "\n",
        "  relevant_memories_label = '\\nRecalled memories and observations'\n",
        "  relevant_memories = agent_components.all_similar_memories.AllSimilarMemories(\n",
        "      model=model,\n",
        "      components={\n",
        "          _get_class_name(observation_summary): observation_summary_label,\n",
        "          _get_class_name(time_display): 'The current date/time is'},\n",
        "      num_memories_to_retrieve=10,\n",
        "      pre_act_key=relevant_memories_label,\n",
        "      logging_channel=measurements.get_channel('AllSimilarMemories').on_next,\n",
        "  )\n",
        "\n",
        "  if config.goal:\n",
        "    goal_label = '\\nOverarching goal'\n",
        "    overarching_goal = agent_components.constant.Constant(\n",
        "        state=config.goal,\n",
        "        pre_act_key=goal_label,\n",
        "        logging_channel=measurements.get_channel(goal_label).on_next)\n",
        "  else:\n",
        "    goal_label = None\n",
        "    overarching_goal = None\n",
        "\n",
        "\n",
        "  question_components = _make_question_components(\n",
        "      agent_name=agent_name,\n",
        "      model=model,\n",
        "      clock=clock,\n",
        "      measurements=measurements\n",
        "  )\n",
        "\n",
        "  core_components = (\n",
        "      instructions,\n",
        "      time_display,\n",
        "      observation,\n",
        "      observation_summary,\n",
        "      relevant_memories,\n",
        "  )\n",
        "\n",
        "  entity_components = core_components + tuple(question_components)\n",
        "  components_of_agent = {\n",
        "      _get_class_name(component): component for component in entity_components\n",
        "  }\n",
        "\n",
        "  components_of_agent[\n",
        "      agent_components.memory_component.DEFAULT_MEMORY_COMPONENT_NAME\n",
        "  ] = agent_components.memory_component.MemoryComponent(raw_memory)\n",
        "  component_order = list(components_of_agent.keys())\n",
        "  if overarching_goal is not None:\n",
        "    components_of_agent[goal_label] = overarching_goal\n",
        "    # Place goal after the instructions.\n",
        "    component_order.insert(1, goal_label)\n",
        "\n",
        "  act_component = agent_components.concat_act_component.ConcatActComponent(\n",
        "      model=model,\n",
        "      clock=clock,\n",
        "      component_order=component_order,\n",
        "      logging_channel=measurements.get_channel('ActComponent').on_next,\n",
        "  )\n",
        "\n",
        "  agent = entity_agent_with_logging.EntityAgentWithLogging(\n",
        "      agent_name=agent_name,\n",
        "      act_component=act_component,\n",
        "      context_components=components_of_agent,\n",
        "      component_logging=measurements,\n",
        "  )\n",
        "\n",
        "  return agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xN_dpScXM98K"
      },
      "outputs": [],
      "source": [
        "import types\n",
        "agent_module = types.SimpleNamespace(__name__= 'custom_agent', build_agent = build_agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTZjLHpYuwOQ"
      },
      "source": [
        "# The simulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNl5UpuHuwOQ"
      },
      "source": [
        "## Initialize the simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCfIjcukuwOQ"
      },
      "outputs": [],
      "source": [
        "# @title Initialize the simulation\n",
        "measurements = measurements_lib.Measurements()\n",
        "runnable_simulation = simulation.Simulation(\n",
        "    model=model,\n",
        "    embedder=embedder,\n",
        "    measurements=measurements,\n",
        "    agent_module=agent_module,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f0j8s-_uwOR"
      },
      "source": [
        "## Run the simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4Z1ttTfuwOR"
      },
      "outputs": [],
      "source": [
        "# @title Run the simulation\n",
        "results_log = runnable_simulation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXb_ay3wLg0k"
      },
      "outputs": [],
      "source": [
        "# @title Display the results log\n",
        "display.HTML(results_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaiIye0KuwOR"
      },
      "source": [
        "## Save the results log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDGkKpsruwOR"
      },
      "outputs": [],
      "source": [
        "# @title Write the results log as an HTML file in the current working directory.\n",
        "filename = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') + '.html'\n",
        "file_handle = open(filename, 'a')\n",
        "file_handle.write(results_log)\n",
        "file_handle.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWDqd4ByzSsT"
      },
      "source": [
        "```\n",
        "Copyright 2023 DeepMind Technologies Limited.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
